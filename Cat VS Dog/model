import tensorflow as tf
import tensorflow.contrib.layers as layers


def inference(images,n_class):
#卷积层
    with tf.variable_scope('conv1') as scope:
        weights=tf.get_variable('weights',
                                shape=[3,3,3,16],
                                initializer=tf.truncated_normal_initializer(stddev=0.1),
                                dtype=tf.float32)
        biases=tf.get_variable('biases',
                               shape=[16],
                               initializer=tf.constant_initializer(0),
                               dtype=tf.float32)
        conv=tf.nn.conv2d(images,weights,strides=[1,1,1,1],padding='SAME')+biases
        conv1=tf.nn.relu(conv,name=scope.name)

    with tf.variable_scope('pooling1_lrn') as scope:
        pool1=tf.nn.max_pool(conv1,
                             ksize=[1,3,3,1],
                             strides=[1,2,2,1],
                             padding='SAME',
                             name='pooling')
        norm1=tf.nn.lrn(pool1,
                        depth_radius=4,
                        bias=0.1,
                        alpha=0.001/9.0,
                        beta=0.75,
                        name='norm1')

    with tf.variable_scope('conv2') as scope:
        weights = tf.get_variable('weights',
                                  shape=[3, 3, 16, 16],
                                  initializer=tf.truncated_normal_initializer(stddev=0.1),
                                  dtype=tf.float32)
        biases = tf.get_variable('biases',
                                 shape=[16],
                                 initializer=tf.constant_initializer(0),
                                 dtype=tf.float32)
        conv = tf.nn.conv2d(norm1, weights, strides=[1, 1, 1, 1], padding='SAME') + biases
        conv2 = tf.nn.relu(conv, name=scope.name)

    with tf.variable_scope('pooling2_lrn') as scope:
        pool2 = tf.nn.max_pool(conv2,
                               ksize=[1, 3, 3, 1],
                               strides=[1, 2, 2, 1],
                               padding='SAME',
                               name='pooling')
        norm2 = tf.nn.lrn(pool2,
                          depth_radius=4,
                          bias=0.1,
                          alpha=0.001 / 9.0,
                          beta=0.75,
                          name='norm2')
#全连接层
    with tf.variable_scope('fc1') as scope:
        reshape=layers.flatten(norm2)
        dim=reshape.get_shape()[1].value#获取拉成行之后的元素个数
        weights=tf.get_variable('weights',
                                shape=[dim,128],
                                initializer=tf.truncated_normal_initializer(stddev=0.005),
                                dtype=tf.float32)
        biases=tf.get_variable('biases',
                               shape=[128],
                               initializer=tf.constant_initializer(0),
                               dtype=tf.float32)

        fc=tf.matmul(reshape,weights)+biases
        fc1=tf.nn.relu(fc,name=scope.name)

        with tf.variable_scope('fc2') as scope:
            weights = tf.get_variable('weights',
                                      shape=[128, 128],
                                      initializer=tf.truncated_normal_initializer(stddev=0.005),
                                      dtype=tf.float32)
            biases = tf.get_variable('biases',
                                     shape=[128],
                                     initializer=tf.constant_initializer(0),
                                     dtype=tf.float32)

            fc = tf.matmul(fc1, weights) + biases
            fc2 = tf.nn.relu(fc, name=scope.name)
#softmax层
        with tf.variable_scope('softmax_linear') as scope:
            weights = tf.get_variable('weights',
                                      shape=[128, n_class],
                                      initializer=tf.truncated_normal_initializer(stddev=0.005),
                                      dtype=tf.float32)
            biases = tf.get_variable('biases',
                                     shape=[n_class],
                                     initializer=tf.constant_initializer(0),
                                     dtype=tf.float32)

            softmax_linear = tf.add(tf.matmul(fc2, weights),biases,name=scope.name)

    return softmax_linear


def loss(logits,labels):
    with tf.variable_scope('loss') as scope:
        cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=labels)
        cost=tf.reduce_mean(cross_entropy)
    return cost

def accuracy(logits,labels):
    with tf.variable_scope('accuracy') as scope:
        accuracy_value=tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(logits,1),tf.cast(labels,tf.int64)),tf.float32))
        return accuracy_value
